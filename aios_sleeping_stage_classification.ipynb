{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GRU, LSTM, Reshape\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define f1 callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        predictions = np.argmax(self.model.predict(X_val), axis=1)\n",
    "        true_labels = np.argmax(y_val, axis=1)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        print(f\"Epoch {epoch + 1}: F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate scheduler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: AISContest_Data\\0\n",
      "Found 2906 files in folder 0.\n",
      "Processing folder: AISContest_Data\\1\n",
      "Found 237 files in folder 1.\n",
      "Processing folder: AISContest_Data\\2\n",
      "Found 1755 files in folder 2.\n",
      "Processing folder: AISContest_Data\\3\n",
      "Found 497 files in folder 3.\n",
      "Processing folder: AISContest_Data\\4\n",
      "Found 418 files in folder 4.\n"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "####################### Please change this to your \n",
    "main_folder = \"AISContest_Data\"\n",
    "\n",
    "# List to store all file paths\n",
    "final_file_list = []\n",
    "\n",
    "# Process folders from 0 to 4\n",
    "for label in range(5):\n",
    "    folder_path = os.path.join(main_folder, str(label))\n",
    "    \n",
    "    if not os.path.exists(folder_path):  # Check if folder exists\n",
    "        print(f\"Warning: Folder {folder_path} does not exist.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing folder: {folder_path}\")\n",
    "    \n",
    "    # Collect all .npy files in the folder\n",
    "    file_list = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".npy\")]\n",
    "    final_file_list.append(file_list)\n",
    "    print(f\"Found {len(file_list)} files in folder {label}.\")\n",
    "\n",
    "# Flatten the list of lists into one list\n",
    "file_list = list(chain.from_iterable(final_file_list))\n",
    "\n",
    "# Load data and labels\n",
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    label = int(os.path.basename(os.path.dirname(file_path)))  # Extract label from folder name\n",
    "    data = np.load(file_path)  # Load data from .npy file\n",
    "    data_list.append(data)\n",
    "    labels_list.append(label)\n",
    "\n",
    "data_array = np.stack(data_list, axis=0)\n",
    "labels_array = np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (5813, 51, 59, 1)\n",
      "Labels shape: (5813,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: {data_array.shape}\")\n",
    "print(f\"Labels shape: {labels_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_array_flat = data_array.reshape(-1, data_array.shape[2])\n",
    "data_array_flat = scaler.fit_transform(data_array_flat)\n",
    "data_array = data_array_flat.reshape(-1, 51, data_array.shape[2], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = data_array.reshape(data_array.shape[0], -1)\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_flat, labels_array)\n",
    "data_array = X_resampled.reshape(-1, 51, data_array.shape[2], 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels_array = to_categorical(y_resampled, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_array, labels_array, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "callbacks = [\n",
    "    LearningRateScheduler(scheduler),\n",
    "    f1_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "##### and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(51, 59, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Reshape((11, -1)),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data and save training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model and Compute f1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute F1-Score\n",
    "f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "print(f\"Final F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot f1 Score over  each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(f1_callback.f1_scores) + 1), f1_callback.f1_scores, marker='o', label='F1-Score')\n",
    "plt.title('F1-Score Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
